#!/usr/bin/env python3
"""
Generate GitHub Pages HTML from filtered news
"""
import json
import os
import logging
import sys
from datetime import datetime
from typing import Dict, List, Any
import re

# æ·»åŠ Pythonè·¯å¾„å¤„ç†
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from utils import load_config


def load_filtered_news() -> List[Dict[str, Any]]:
    """
    åŠ è½½ç­›é€‰åçš„æ–°é—»æ•°æ®
    ä»output/filtered_news.jsonæ–‡ä»¶ä¸­è¯»å–å·²ç­›é€‰çš„æ–°é—»æ•°æ®
    å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨æˆ–ä¸ºç©ºï¼Œè¿”å›ç©ºåˆ—è¡¨
    @return {List[Dict[str, Any]]} æ–°é—»æ•°æ®åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸ºåŒ…å«æ–°é—»ä¿¡æ¯çš„å­—å…¸
    """
    # æ–°é—»æ•°æ®æ–‡ä»¶è·¯å¾„
    news_file = "output/filtered_news.json"
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(news_file):
        logging.warning(f"æ–°é—»æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {news_file}")
        return []
    # è¯»å–å¹¶è¿”å›JSONæ•°æ®
    with open(news_file, 'r', encoding='utf-8') as f:
        try:
            return json.load(f)
        except json.JSONDecodeError:
            logging.error(f"æ–°é—»æ•°æ®æ–‡ä»¶æ ¼å¼é”™è¯¯: {news_file}")
            return []


def extract_keywords_from_text(text: str, keywords: List[str]) -> List[str]:
    """
    ä»æ–‡æœ¬ä¸­æå–åŒ¹é…çš„å…³é”®è¯
    å°†æ–‡æœ¬å’Œå…³é”®è¯éƒ½è½¬ä¸ºå°å†™åè¿›è¡ŒåŒ¹é…ï¼Œé¿å…å¤§å°å†™æ•æ„Ÿé—®é¢˜
    @param {str} text - éœ€è¦æå–å…³é”®è¯çš„æ–‡æœ¬å†…å®¹
    @param {List[str]} keywords - å…³é”®è¯åˆ—è¡¨
    @return {List[str]} åŒ¹é…åˆ°çš„å…³é”®è¯åˆ—è¡¨
    """
    # å°†æ–‡æœ¬è½¬ä¸ºå°å†™ï¼Œå®ç°å¤§å°å†™ä¸æ•æ„ŸåŒ¹é…
    text_lower = text.lower()
    matched_keywords = []
    # éå†æ‰€æœ‰å…³é”®è¯ï¼Œæ£€æŸ¥æ˜¯å¦åœ¨æ–‡æœ¬ä¸­å‡ºç°
    for keyword in keywords:
        if keyword.lower() in text_lower:
            matched_keywords.append(keyword)
    return matched_keywords


def group_news_by_keywords(news_list: List[Dict[str, Any]], keywords: List[str]) -> Dict[str, List[Dict[str, Any]]]:
    """
    å°†æ–°é—»æŒ‰åŒ¹é…çš„å…³é”®è¯åˆ†ç»„
    ç»„åˆæ–°é—»æ ‡é¢˜ã€æè¿°å’Œå†…å®¹ï¼Œæå–åŒ¹é…çš„å…³é”®è¯ï¼Œå¹¶å°†æ–°é—»å½’ç±»åˆ°å¯¹åº”çš„å…³é”®è¯ç»„
    @param {List[Dict[str, Any]]} news_list - æ–°é—»åˆ—è¡¨
    @param {List[str]} keywords - å…³é”®è¯åˆ—è¡¨
    @return {Dict[str, List[Dict[str, Any]]]} æŒ‰å…³é”®è¯åˆ†ç»„çš„æ–°é—»å­—å…¸
    """
    keyword_groups = {}
    for news in news_list:
        # æå–æ–°é—»çš„æ ‡é¢˜ã€æè¿°å’Œå†…å®¹
        title = news.get('title', '')
        description = news.get('description', '')
        content = news.get('content', '')
        # ç»„åˆæ‰€æœ‰æ–‡æœ¬ç”¨äºå…³é”®è¯åŒ¹é…
        full_text = f"{title} {description} {content}"
        matched_keywords = extract_keywords_from_text(full_text, keywords)
        # å°†æ–°é—»æ·»åŠ åˆ°æ¯ä¸ªåŒ¹é…çš„å…³é”®è¯ç»„
        for keyword in matched_keywords:
            if keyword not in keyword_groups:
                keyword_groups[keyword] = []
            keyword_groups[keyword].append(news)
    return keyword_groups


def format_date(date_str: str) -> str:
    """Format date string for display (ç»Ÿä¸€ä¸ºYYYY-MM-DDæ ¼å¼)"""
    try:
        # Handle different date formats
        if 'T' in date_str:
            dt = datetime.fromisoformat(date_str.replace('Z', '+00:00'))
        else:
            dt = datetime.strptime(date_str, '%a, %d %b %Y %H:%M:%S %z')
        return dt.strftime('%Y-%m-%d')
    except:
        return date_str


def truncate_text(text: str, max_length: int = 200) -> str:
    """Truncate text to specified length"""
    if len(text) <= max_length:
        return text
    return text[:max_length] + '...'


def clean_html(text: str) -> str:
    """Clean HTML tags from text"""
    if not text:
        return ""
    # Remove HTML tags
    clean = re.compile('<.*?>')
    return re.sub(clean, '', text)


def generate_html(news_data: List[Dict[str, Any]], keywords: List[str]) -> str:
    """Generate HTML content for GitHub Pages"""
    keyword_groups = group_news_by_keywords(news_data, keywords)
    html_content = """<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç§‘æŠ€æ–°é—»èšåˆ - RSSç²¾é€‰</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        body {
            font-family: -apple-system, BlinkMacSystemFont, 
                        'Segoe UI', 'PingFang SC', 
                        'Hiragino Sans GB', 'Microsoft YaHei', 
                        sans-serif;
            line-height: 1.6;
            color: #333;
            background-color: #f5f5f5;
        }
        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
        }
        header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px 0;
            text-align: center;
            margin-bottom: 30px;
            border-radius: 10px;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
        }
        h1 {
            font-size: 2.5em;
            margin-bottom: 10px;
        }
        .subtitle {
            font-size: 1.2em;
            opacity: 0.9;
        }
        .stats {
            display: flex;
            justify-content: center;
            gap: 40px;
            margin-top: 20px;
            flex-wrap: wrap;
        }
        .stat-item {
            text-align: center;
        }
        .stat-number {
            font-size: 2em;
            font-weight: bold;
            display: block;
        }
        .keyword-section {
            background: white;
            margin-bottom: 30px;
            border-radius: 10px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            overflow: hidden;
        }
        .keyword-header {
            background: #4CAF50;
            color: white;
            padding: 15px 20px;
            font-size: 1.3em;
            font-weight: bold;
        }
        .news-list {
            padding: 0;
        }
        .news-item {
            padding: 20px;
            border-bottom: 1px solid #eee;
            transition: background-color 0.3s;
        }
        .news-item:hover {
            background-color: #f9f9f9;
        }
        .news-item:last-child {
            border-bottom: none;
        }
        .news-title {
            font-size: 1.2em;
            font-weight: bold;
            margin-bottom: 8px;
        }
        .news-title a {
            color: #2c3e50;
            text-decoration: none;
        }
        .news-title a:hover {
            color: #667eea;
            text-decoration: underline;
        }
        .news-meta {
            color: #666;
            font-size: 0.9em;
            margin-bottom: 10px;
        }
        .news-description {
            color: #555;
            line-height: 1.5;
            margin-bottom: 10px;
        }
        .news-tags {
            display: flex;
            gap: 8px;
            flex-wrap: wrap;
        }
        .tag {
            background: #e3f2fd;
            color: #1976d2;
            padding: 4px 8px;
            border-radius: 12px;
            font-size: 0.8em;
        }
        .source-tag {
            background: #fff3e0;
            color: #f57c00;
        }
        .category-tag {
            background: #e8f5e8;
            color: #388e3c;
        }
        .footer {
            text-align: center;
            padding: 40px 0;
            color: #666;
            border-top: 1px solid #ddd;
            margin-top: 40px;
        }
        .update-time {
            background: white;
            padding: 15px;
            border-radius: 10px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            text-align: center;
            margin-bottom: 20px;
        }
        @media (max-width: 768px) {
            .container {
                padding: 5px;
            }
            h1 {
                font-size: 1.8em;
            }
            .subtitle {
                font-size: 1em;
            }
            .stats {
                gap: 15px;
                flex-direction: column;
                align-items: center;
            }
            .stat-item {
                margin-bottom: 10px;
                width: 100%;
                background: #f5f5f5;
                padding: 10px;
                border-radius: 8px;
            }
            .stat-number {
                font-size: 1.5em;
            }
            .keyword-header {
                padding: 12px 15px;
                font-size: 1.1em;
            }
            .news-item {
                padding: 12px;
            }
            .news-title {
                font-size: 1.1em;
            }
            .news-description {
                font-size: 0.9em;
            }
            .news-tags {
                flex-wrap: wrap;
                gap: 5px;
            }
            .tag {
                padding: 3px 6px;
                font-size: 0.75em;
            }
        }
        @media (max-width: 480px) {
            h1 {
                font-size: 1.5em;
            }
            .news-title {
                font-size: 1em;
            }
            .news-meta {
                font-size: 0.8em;
                display: flex;
                flex-direction: column;
                gap: 5px;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>ç§‘æŠ€æ–°é—»èšåˆ</h1>
            <p class="subtitle">åŸºäºå…³é”®è¯çš„æ™ºèƒ½æ–°é—»ç­›é€‰ä¸èšåˆ</p>
            <div class="stats">
                <div class="stat-item">
                    <span class="stat-number">""" + str(len(news_data)) + """</span>
                    <span>ç¯‡ç²¾é€‰æ–‡ç« </span>
                </div>
                <div class="stat-item">
                    <span class="stat-number">""" + str(len(keyword_groups)) + """</span>
                    <span>ä¸ªå…³é”®è¯</span>
                </div>
                <div class="stat-item">
                    <span class="stat-number">""" + str(len(set([news.get('source', 'Unknown') for news in news_data]))) + """</span>
                    <span>ä¸ªæ¥æº</span>
                </div>
            </div>
        </header>
        <div class="update-time">
            <strong>æœ€åæ›´æ–°ï¼š</strong>""" + datetime.now().strftime('%Y-%m-%d %H:%M:%S') + """ (UTC+8)
        </div>
"""
    # Add keyword sections
    for keyword, news_list in sorted(keyword_groups.items(), key=lambda x: len(x[1]), reverse=True):
        html_content += f"""
        <div class="keyword-section">
            <div class="keyword-header">
                {keyword} ({len(news_list)} ç¯‡)
            </div>
            <div class="news-list">
"""
        for news in news_list:
            title = clean_html(news.get('title', 'æ— æ ‡é¢˜'))
            link = news.get('link', '#')
            description = clean_html(news.get('description', ''))
            source = news.get('source', 'æœªçŸ¥æ¥æº')
            category = news.get('category', 'æœªåˆ†ç±»')
            published_date = news.get('published')
            published = format_date(published_date) if published_date else "æ—¥æœŸç¼ºå¤±"
            html_content += f"""
                <div class="news-item">
                    <div class="news-title">
                        <a href="{link}" target="_blank" rel="noopener noreferrer">{title}</a>
                    </div>
                    <div class="news-meta">
                        ğŸ“… {published} | ğŸ¢ {source}
                    </div>
                    <div class="news-description">
                        {truncate_text(description, 300)}
                    </div>
                    <div class="news-tags">
                        <span class="tag source-tag">{source}</span>
                        <span class="tag category-tag">{category}</span>
                    </div>
                </div>
"""
        html_content += """
            </div>
        </div>
"""
    html_content += """
        <div class="footer">
            <p>ç”± RSS æ–°é—»èšåˆå™¨è‡ªåŠ¨ç”Ÿæˆ | æ•°æ®æ¥æºï¼šå„å¤§ç§‘æŠ€åª’ä½“ RSS è®¢é˜…æº</p>
            <p>æ›´æ–°æ—¶é—´ï¼šæ¯6å°æ—¶è‡ªåŠ¨æ›´æ–°ä¸€æ¬¡</p>
        </div>
    </div>
</body>
</html>
"""
    return html_content


def save_html_to_pages(html_content: str) -> bool:
    """
    å°†HTMLå†…å®¹ä¿å­˜åˆ°GitHub Pagesæ‰€éœ€çš„ç›®å½•
    ä¿å­˜è·¯å¾„ä¸ºdocs/index.htmlï¼Œè¯¥ç›®å½•æ˜¯GitHub Pagesé»˜è®¤çš„å‘å¸ƒç›®å½•
    å¦‚æœç›®å½•ä¸å­˜åœ¨ä¼šè‡ªåŠ¨åˆ›å»º
    @param {str} html_content - è¦ä¿å­˜çš„HTMLå†…å®¹
    @return {bool} ä¿å­˜æˆåŠŸè¿”å›Trueï¼Œå¤±è´¥è¿”å›False
    """
    try:
        # åˆ›å»ºdocsç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        # GitHub Pagesé»˜è®¤ä½¿ç”¨docsç›®å½•ä½œä¸ºå‘å¸ƒæº
        os.makedirs("docs", exist_ok=True)
        # ä¿å­˜HTMLå†…å®¹åˆ°docs/index.html
        with open("docs/index.html", "w", encoding="utf-8") as f:
            f.write(html_content)
        logging.info("âœ… GitHub Pages HTML å·²ç”Ÿæˆå¹¶ä¿å­˜åˆ° docs/index.html")
        return True
    except Exception as e:
        logging.error(f"ä¿å­˜HTMLæ–‡ä»¶å¤±è´¥: {str(e)}")
        return False


def main():
    """
    ä¸»å‡½æ•°ï¼šç”ŸæˆGitHub Pagesçš„HTMLé¡µé¢
    æµç¨‹ï¼š
    1. åŠ è½½ç­›é€‰åçš„æ–°é—»æ•°æ®
    2. åŠ è½½å…³é”®è¯é…ç½®
    3. ç”ŸæˆHTMLå†…å®¹
    4. ä¿å­˜HTMLåˆ°docsç›®å½•
    å¦‚æœ‰ä»»ä½•æ­¥éª¤å¤±è´¥ï¼Œè®°å½•é”™è¯¯å¹¶é€€å‡ºç¨‹åº
    """
    # é…ç½®loggingæ¨¡å—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler()
        ]
    )
    try:
        logging.info("å¼€å§‹ç”ŸæˆGitHub Pages...")
        # åŠ è½½ç­›é€‰åçš„æ–°é—»æ•°æ®
        news_data = load_filtered_news()
        # æ£€æŸ¥æ˜¯å¦æœ‰æ–°é—»æ•°æ®
        if not news_data:
            logging.warning("æ²¡æœ‰æ–°é—»æ•°æ®å¯ä¾›ç”Ÿæˆé¡µé¢")
            return
        # åŠ è½½å…³é”®è¯é…ç½®
        try:
            # å…³é”®è¯é…ç½®æ–‡ä»¶è·¯å¾„ï¼šconfig/keywords.yaml
            keywords_config = load_config('config/keywords.yaml')
        except json.JSONDecodeError:
            logging.error("å…³é”®è¯é…ç½®æ–‡ä»¶JSONæ ¼å¼é”™è¯¯ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶è¯­æ³•")
            sys.exit(1)
        except Exception as e:
            logging.error(f"åŠ è½½å…³é”®è¯é…ç½®å¤±è´¥: {str(e)}")
            sys.exit(1)
        # æå–éœ€è¦åŒ¹é…çš„å…³é”®è¯åˆ—è¡¨
        keywords = keywords_config.get('include_keywords', [])
        if not keywords:
            logging.warning("æœªé…ç½®ä»»ä½•å…³é”®è¯ï¼Œå°†æ— æ³•æŒ‰å…³é”®è¯åˆ†ç»„")
        # ç”ŸæˆHTMLå†…å®¹
        try:
            html_content = generate_html(news_data, keywords)
        except Exception as e:
            logging.error(f"ç”ŸæˆHTMLå†…å®¹å¤±è´¥: {str(e)}")
            sys.exit(1)
        # ä¿å­˜HTMLåˆ°GitHub Pagesç›®å½•
        if save_html_to_pages(html_content):
            logging.info("GitHub Pagesç”ŸæˆæˆåŠŸï¼Œæ–‡ä»¶å·²ä¿å­˜åˆ°docs/index.html")
        else:
            logging.error("GitHub Pagesç”Ÿæˆå¤±è´¥")
            sys.exit(1)
    except Exception as e:
        logging.error(f'ç”ŸæˆHTMLé¡µé¢å¤±è´¥: {str(e)}')
        sys.exit(1)


if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        logging.error(f'ç”ŸæˆHTMLé¡µé¢å¤±è´¥: {str(e)}')
        sys.exit(1)
